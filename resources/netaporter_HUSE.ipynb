{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "netaporter-HUSE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Am3i4SLL13V",
        "colab_type": "text"
      },
      "source": [
        "# HUSE: Hierarchical Universal Semantic Embeddings\n",
        "\n",
        "```\n",
        "This is my implementation of the HUSE paper in PyTorch, as part of an assessment assignment at GreenDeck.\n",
        "\n",
        "https://arxiv.org/pdf/1911.05978.pdf\n",
        "This code is also available at: https://github.com/arpytanshu/HUSE-PyTorch\n",
        "\n",
        "author: arpytanshu@gmail.com\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-Ka2BAyHs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_NYuBSeSZJg",
        "colab_type": "code",
        "outputId": "3e3c7a9b-4b6d-4576-e4f7-093b0e454f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "from torch.optim import RMSprop\n",
        "from torchvision import transforms\n",
        "from transformers import BertModel, BertConfig\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, BertConfig"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gv5feF0L4Bj",
        "colab_type": "code",
        "outputId": "a504fb16-d095-42dc-8197-82d9f0800f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ======== === ====== #\n",
        "# NOTEBOOK RUN CONFIG #\n",
        "# ======== === ====== #\n",
        "\n",
        "DEV_MODE = False  # for development runs only\n",
        "NUM_DEV_SAMPLES = 0 # number of samples to use in DEV_MODE\n",
        "\n",
        "# IMAGE_ARCHIVE_PATH = '/content/drive/My Drive/TEMP/HUSE/val_images.zip'\n",
        "# DATAFRAME_PATH = '/content/drive/My Drive/TEMP/HUSE/validation_data.csv'\n",
        "# IMAGE_PATH = '/content/dataset_images/content/sample_data/val_images/'\n",
        "\n",
        "IMAGE_PATH = '/content/dataset_images/netaporter_gb/'\n",
        "IMAGE_ARCHIVE_PATH = '/content/drive/My Drive/TEMP/HUSE/netaporter_gb.zip'\n",
        "DATAFRAME_PATH = '/content/drive/My Drive/TEMP/HUSE/training_data.csv'\n",
        "\n",
        "VAL_SPLIT = 0.1\n",
        "BERT_MODEL_NAME = 'bert-base-uncased'\n",
        "OPTIM_LEARNING_RATE = 1.6192e-05\n",
        "OPTIM_MOMENTUM = 0.9\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64 if DEV_MODE else 512\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "print('Device Type: ', device.type)\n",
        "if device.type=='cuda':\n",
        "  dtype = torch.cuda.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Type:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ5aaCThM391",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with zipfile.ZipFile(IMAGE_ARCHIVE_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset_images/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F618TPCXTgio",
        "colab_type": "text"
      },
      "source": [
        "#  \n",
        "# **Data**\n",
        "50k products accompanied along with its image, text and class name\n",
        "\n",
        "To apply transforms as we feed data into the model, we make a PyTorch DataSet.  \n",
        "#  \n",
        "#### **The train Dataset object from the can be queried later to get the following:**\n",
        "- A tfidf-vectorizer fitted on all text sample. { self.tokenizer }\n",
        "- Mapping from class_names to class_index\n",
        "#  \n",
        "\n",
        "#### **The Dataset reads in the image, text and labels for each sample and applies the following transformations to them:**  \n",
        "- Transformations on Image\n",
        "  - Resize [ to 224x224, (as expected by pre-trained torchvision models) ]\n",
        "  - Random Contrast, Brightness, Hue, Saturation\n",
        "  - Random Horizontal Flip\n",
        "  - Normalizes each channel (as expected by pre-trained torchvision models)\n",
        "- Cleans text { **!TODO** }\n",
        "- Tokenizes the text and makes input_ids for getting BERT embeddings for text\n",
        "- Gets the tfidf vector for text\n",
        "#  \n",
        "\n",
        "#### **The \\_\\_getitem__ method for the Dataset returns the following for each sample:** \n",
        "- image : torch tensor of shape 3 x 224 x 224\n",
        "- bert_input_ids : torch tensor ( input ids corresponding to text tokens, created by BertTokenizer)\n",
        "- tfidf_vector : torch tensor (tfidf feature vectors of shape self.tokenizer.num_words)\n",
        "- labels : torch tensor ( index corresponding to the class name )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpyPgiTVUuh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NAPDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, image_path,\n",
        "                 train_mode = True,\n",
        "                 bert_model_name = 'bert-base-uncased',\n",
        "                 transforms = None,\n",
        "                 tokenizer = None):\n",
        "        assert(not (train_mode==False and tokenizer==None)), \\\n",
        "            'tokenizer must be provided to gdDataset if train_mode == False'\n",
        "        self.df = df\n",
        "        self.bert_max_length = 25\n",
        "        self.train_mode = train_mode\n",
        "        self.image_path = image_path\n",
        "        self.transforms = transforms\n",
        "        self.bert_model_name = bert_model_name\n",
        "        self.tokenizer = tokenizer if tokenizer else self._get_tfidf_vectorizer()\n",
        "        self.bert_tokenizer = BertTokenizer.from_pretrained(self.bert_model_name)\n",
        "        self.num_classes = self.df.classes.nunique()\n",
        "        self.classes_map = self._get_label_map()\n",
        "        self.df['classes_map'] = df.classes.map(self.classes_map)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'text']\n",
        "        # TODO : text = self.clean_text(text)\n",
        "        tfidf_vector = self._get_tfidf_vector(text)\n",
        "        bert_input_ids = self._get_bert_input_ids(text)\n",
        "        label = self.df.loc[idx, 'classes_map'] if self.train_mode else None\n",
        "        image_path = self.image_path + self.df.loc[idx, 'image']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        return image, bert_input_ids, tfidf_vector, label\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        # TODO : add text cleaning methods here\n",
        "        pass\n",
        "\n",
        "    def _get_bert_input_ids(self, text):\n",
        "        # returns the input_ids for BertModel using BertTokenizer\n",
        "        input_ids = self.bert_tokenizer.encode(text,\n",
        "                                          return_tensors='pt',\n",
        "                                          pad_to_max_length=True,\n",
        "                                          max_length=self.bert_max_length)\n",
        "        input_ids.squeeze_(0)\n",
        "        return input_ids\n",
        "    \n",
        "    def _get_tfidf_vector(self, text):\n",
        "        '''\n",
        "        Uses Keras.preprocessing.text.Tokenizer object created by _get_tfidf_vectorizer\n",
        "        to generate tfidf feature vectors for samples.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tfidf_vector : torch tensor\n",
        "        '''\n",
        "        tfidf_vector = self.tokenizer.texts_to_matrix([text], mode='tfidf')\n",
        "        tfidf_vector = torch.tensor(tfidf_vector, dtype=torch.float32).squeeze(0)\n",
        "        return tfidf_vector\n",
        "    \n",
        "    def _get_tfidf_vectorizer(self):\n",
        "        '''\n",
        "        Should be used only when using trainind data.\n",
        "        Fits a tokenizer object on the texts to create tfidf vectors.\n",
        "        The vectorizer skips tokens that appear only once in the corpus.\n",
        "            i.e. to be a valid term in the tf-idf matrix,\n",
        "            a term must appear atleast twice across all documents.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        tokenizer : keras.preprocessing.text.tokenizer\n",
        "        '''\n",
        "        corpus = list(self.df.text)\n",
        "        # fit a tokenizer to get vocab size\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "        vocab_size = len([k for k,v in tokenizer.word_counts.items() if v > 1])\n",
        "        # fit tokenizer with reduced vocabulary\n",
        "        tokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "        return tokenizer\n",
        "    \n",
        "    def _get_label_map(self):\n",
        "        '''\n",
        "        Creates a mapping from class labels to integer ids.\n",
        "        The mapping is used for creating the Semantic Graph later.\n",
        "        Returns\n",
        "        -------\n",
        "        A dictionary mapping from class names to integers.\n",
        "        '''\n",
        "        _, class_names = self.df.classes.factorize(sort=True)\n",
        "        mapping = {k:v for v,k in enumerate(class_names)}\n",
        "        return mapping\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return 'len:{} classes:{}'.format(len(self.df), self.num_classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.__len__()\n",
        "\n",
        "\n",
        "torchvision_models_mean = [0.485, 0.456, 0.406]\n",
        "torchvision_models_stDev = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(torchvision_models_mean, torchvision_models_stDev)\n",
        "        ]), \n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 244)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(torchvision_models_mean, torchvision_models_stDev),\n",
        "        ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 244)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(torchvision_models_mean, torchvision_models_stDev),\n",
        "        ])\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68CEEXxrbNcM",
        "colab_type": "text"
      },
      "source": [
        "# **Semantic Graph**\n",
        "\n",
        "To create a Semantic Graph, we take in the class_name to class_index mapping.\n",
        "\n",
        "This mapping should be obtained from the Dataset object.\n",
        "```\n",
        "tr_dataset = utils.NAPDataset(tr_df.copy(),\n",
        "                          image_path = IMAGE_PATH,\n",
        "                          train_mode = True,\n",
        "                          bert_model_name = BERT_MODEL_NAME,\n",
        "                          transforms = utils.image_transforms['train'],\n",
        "                          tokenizer = None)\n",
        "classes_map = tr_dataset.classes_map\n",
        "\n",
        "classes_map\n",
        "{ 'accessories<belts<wide': 0,\n",
        "  'accessories<books<books': 1,\n",
        " ...\n",
        "  'shoes<pumps<high heel': 41,\n",
        "  'shoes<pumps<mid heel': 42\n",
        " ...\n",
        "}\n",
        "\n",
        "A = SemanticGraph(classes_map)\n",
        "\n",
        "```\n",
        "\n",
        "### Getting Class name Embeddings  \n",
        "To get the Embeddings for each class name, we use Bert Embeddings for the 3 level of hierarchy in class names, and concatenate them together.  \n",
        "If a hierarchy has multiple words, its embeddings are mean pooled across the tokens.\n",
        "![semantic_graph_embedding](https://raw.githubusercontent.com/arpytanshu/HUSE-PyTorch/master/resources/semantic_graph_embedding.jpg)\n",
        "\n",
        "Once we get the Embeddings for the class_names, we fill in the Semantic Graph at\n",
        "position A[i][j] with cosine distance between embeddings of class_name[i] & class_name[j].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu32OTo6bSgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SemanticGraph(classes_map):\n",
        "    '''\n",
        "    Creates a Semantic Similarity Graph using cosine distance between two class name embeddings\n",
        "    The Class Name Embeddings are created using Bert Pre-trained model.\n",
        "    The embeddings from the second last layer is mean-pooled.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    classes_map : dict\n",
        "        dictionary mapping class names to their integer ids\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Semantic Graph\n",
        "    A torch tensor of dimension [ num_classes x num_classes x embedding_dim ]\n",
        "    '''\n",
        "\n",
        "    BE = BertSentenceEncoder('bert-base-uncased')\n",
        "    num_classes = classes_map.__len__()\n",
        "    embed_dim = 3 * BE.model.pooler.dense.in_features\n",
        "    \n",
        "    # A 0 tensor for the semantic graph { S_G }\n",
        "    S_G = torch.zeros((num_classes, num_classes), dtype=torch.float32)\n",
        "    \n",
        "    # to temporarily hold the class names embeddings { CNE }\n",
        "    CNE = torch.zeros((num_classes, embed_dim), dtype = torch.float32)\n",
        "    \n",
        "    # get embeddings for all class names\n",
        "    for class_name,index in classes_map.items():\n",
        "        class_name = _fix_class_name(class_name)\n",
        "        CNE[index] = BE.encoder(class_name, pooler='mean').reshape(-1) # expected dimension => [3 x 768] = [2304]\n",
        "    \n",
        "    # create the Semantic Graph using cosine distance between class name embeddings\n",
        "    for iy in range(num_classes):\n",
        "        for ix in range(iy, num_classes):\n",
        "            cos_sim = F.cosine_similarity(CNE[iy].reshape(1,-1), CNE[ix].reshape(1,-1))\n",
        "            S_G[ix][iy] = S_G[iy][ix] = cos_sim\n",
        "    return S_G"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnSqVaFhgm9_",
        "colab_type": "text"
      },
      "source": [
        "#### The following is a class to help extract Embeddings for class names and mean pool them along the tokens.  \n",
        "#### It uses Pre-Trained BertModel from HuggingFace Transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOkKAau-eHD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSentenceEncoder():\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        '''\n",
        "        Uses a pre-trained bert to embed sentences and pool them along the tokens.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name : string, optional\n",
        "            DESCRIPTION. The default is 'bert-base-cased'.\n",
        "            \n",
        "            Find a list of usable pre-trained bert models from:\n",
        "                https://huggingface.co/transformers/pretrained_models.html\n",
        "        '''\n",
        "\n",
        "        self.model_name =   model_name\n",
        "        self.config =       BertConfig.from_pretrained(self.model_name, output_hidden_states=True, training=False)\n",
        "        self.model =        BertModel.from_pretrained(self.model_name, config=self.config)\n",
        "        self.tokenizer =    BertTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
        "        self.device =       torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "        self.model.requires_grad_(False)\n",
        "        self.model = self.model.to(self.device)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return 'BertSentenceEncoder model:{}'.format(self.model_name)\n",
        "    \n",
        "    def _mean_pooler(self, encoding):\n",
        "        return encoding.mean(dim=1)\n",
        "    \n",
        "    def _max_pooler(self, encoding):\n",
        "        return encoding.max(dim=1).values\n",
        "    \n",
        "    def encoder(self, sentences, layer=-2, max_length=20, pooler='mean' ):\n",
        "     \n",
        "        assert isinstance(sentences, list), \\\n",
        "            \"parameter 'sentences' is supposed to be a list of string/s\"\n",
        "        assert all(isinstance(x, str) for x in sentences), \\\n",
        "            \"parameter 'sentences' must contain strings only\"\n",
        "        \n",
        "        '''\n",
        "        model(input_tokens) returns a tuple of 3 elements.\n",
        "        out[0] : last_hidden_state  of shape [ B x T x D ]\n",
        "        out[1] : pooler_output      of shape [ B x D ]\n",
        "        out[2] : hidden_states      13 tuples, one for each hidden layer\n",
        "                                    each tuple of shape [ B x T x D ]        \n",
        "        '''\n",
        "        with torch.no_grad():\n",
        "            input_ids = self.tokenizer.batch_encode_plus(sentences, return_tensors='pt', pad_to_max_length=True, max_length=max_length)['input_ids']\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            encoded = self.model(input_ids)\n",
        "        if pooler == 'max':\n",
        "            pooling_fn = self._max_pooler\n",
        "        else: # anythig else defaults to mean-pooling\n",
        "            pooling_fn = self._mean_pooler\n",
        "        pooled = pooling_fn(encoded[2][layer])\n",
        "        return pooled\n",
        "\n",
        "\n",
        "\n",
        "def _fix_class_name(sample):\n",
        "    sample = sample.replace('-', ' ').split('<')\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZZBI9GBi02y",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-trained Models**\n",
        "\n",
        "For getting Image & Text embeddings for the data, we use pre-trained models from **torchvision** and **huggingface's transformers**.\n",
        "\n",
        "A list of model names are added in ```pretrained_imagenet_models``` and ```pretrained_bert_models```.  \n",
        "\n",
        "\n",
        "We furthur define 2 methods named ```ImageEmbeddingModel``` & ```BertEmbeddingModel```.  \n",
        "These methods\n",
        "- Return a pretrained image model and pertrained bert model respectively.\n",
        "- All parameters for both of these models are frozen.\n",
        "- They return an integer ```out_dimension```, which is the size of the embedding that the model is expected to return.\n",
        "\n",
        "These methods can be easily modified to support different models as well, without changing the rest of the code.\n",
        "\n",
        "# <br>\n",
        "\n",
        "**NOTE:**  \n",
        "**The HUSE paper described the text description for each sample (text modal)\n",
        "to be much greater than 512 tokens (which is the limit for BERT) for the dataset it used { UPMC Food-101 } and thus required to extract 512 most important tokens from the text description.**\n",
        "\n",
        "**However the textual description of the samples in the dataset shared with us is \n",
        "much smaller than 512, thus eliminating the need for such token clipping.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXySYnZ7jRMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pretrained_imagenet_models = {\n",
        "    'resnet18'      : models.resnet18,\n",
        "    'resnet101'     : models.resnet101,\n",
        "    'resnet152'     : models.resnet152,\n",
        "    'resnext101'    : models.resnext101_32x8d,\n",
        "    'resnext50'     : models.resnext50_32x4d,\n",
        "    'default'       : models.resnet18\n",
        "    }\n",
        "\n",
        "\n",
        "pretrained_bert_models = [  'bert-base-cased',\n",
        "                            'bert-large-cased',\n",
        "                            'bert-base-uncased',\n",
        "                            'bert-large-uncased',\n",
        "                            ]\n",
        "\n",
        "\n",
        "'''\n",
        "Methods for getting model for Image Embeddings\n",
        "We use pre-trained models trained on ImageNet from torchvision\n",
        "to get 1000-Dimentional vector representation for images.\n",
        "'''\n",
        "def ImageEmbeddingModel(model_name):\n",
        "    '''\n",
        "    Returns a PyTorch pre-trained model.\n",
        "    Valid model names from torchvision can be added to pretrained_models dictionary.    \n",
        "    Parameters\n",
        "    ----------    \n",
        "    model_name : string\n",
        "        A valid model name having an entry in the pretrained_models dictionary.\n",
        "        If the model_name provided is not valid, a default is used.        \n",
        "    features_extract : bool, optional\n",
        "         If True, sets requires_grad attribute of model's parameter to False                    \n",
        "    '''\n",
        "    if model_name not in pretrained_imagenet_models.keys():\n",
        "        model_name = 'default'\n",
        "        print('Invalid model name for ImageEmbeddingModel. Using default {}'.\\\n",
        "              format(pretrained_imagenet_models.get('default')))        \n",
        "\n",
        "    model = pretrained_imagenet_models.get(model_name)(pretrained=True)    \n",
        "    # freeze model parameters\n",
        "    model.requires_grad_(False)\n",
        "    out_dimension = model.fc.out_features\n",
        "    return model, out_dimension\n",
        "\n",
        "\n",
        "'''\n",
        "Methods / Classes for getting BERT Embeddings text.\n",
        "'''\n",
        "def BertEmbeddingModel(model_name):\n",
        "    '''\n",
        "    Returns a PyTorch pre-trained model.    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model_name : string\n",
        "        A valid bert model name from huggingface's transformers.\n",
        "        see: https://huggingface.co/transformers/pretrained_models.html\n",
        "    Returns\n",
        "    -------\n",
        "    bert_model : A transformers Pre trained Bert model        \n",
        "        usage:\n",
        "        ------    \n",
        "        out = bert_model(input_tokens)        \n",
        "        input_tokens is a pytorch tensor of tokenized sentences that are to be embedded.\n",
        "            see : BertTokenizer            \n",
        "        out is a tuple of 3 elements.\n",
        "            out[0] : last_hidden_state  of shape [ B x T x D ]\n",
        "            out[1] : pooler_output      of shape [ B x D ]\n",
        "            out[2] : hidden_states      13 tuples, one for each hidden layer\n",
        "                                    each tuple of shape [ B x T x D ] .                                    \n",
        "                                    B : batch_size\n",
        "                                    T : sequence_length / time_dimension\n",
        "                                    D : hidden_size [ 768 for base model, 1024 for large model ]                                    \n",
        "                                    out[2][-1] : embeddings from last layer\n",
        "                                    out[2][1] : embeddings from first layer\n",
        "    '''\n",
        "    if model_name not in pretrained_bert_models:\n",
        "        print('Invalid model name for BertEmbeddingModel. Using default bert-base-cased.')\n",
        "        model_name = 'bert-base-cased'\n",
        "    bert_config =       BertConfig.from_pretrained(model_name, output_hidden_states=True, training=False)\n",
        "    bert_model =        BertModel.from_pretrained(model_name, config=bert_config)\n",
        "    # freeze parameters\n",
        "    bert_model.requires_grad_(False)\n",
        "    out_dimension = bert_config.hidden_size\n",
        "    return bert_model, out_dimension\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk4vb4cknQmx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The function ```bert_embeddings_pooler``` is a helper method, which concatenates bert embeddings from the specified number ( num_layers_to_use ) of layers & mean pools them along the tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zNfSE6QnRhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _mean_pooler(encoding):\n",
        "    return encoding.mean(dim=1)\n",
        "    \n",
        "def bert_embeddings_pooler(bert_out, num_layers_to_use):\n",
        "    '''\n",
        "    Concatenates embeddings from last N ( = num_layers_to_usehidden ) hidden layer states.\n",
        "    The embedding for each layer is mean pooled along the time / sequence dimension.\n",
        "    Example\n",
        "    -------\n",
        "    This concatenate the embeddings from the last num_layers_to_use layers for each token \n",
        "    and then average all token embeddings for a sequence.\n",
        "        out = BertModel(input_ids)\n",
        "        embeddings = bert_embeddings_pooler(out, 4)\n",
        "    '''\n",
        "    hidden_states = bert_out[2]\n",
        "    layers_to_use = range(-num_layers_to_use, 0)    \n",
        "    pooled = None\n",
        "    for layer in layers_to_use:\n",
        "        if pooled is None:\n",
        "            pooled = _mean_pooler(hidden_states[layer])\n",
        "        else:\n",
        "            pooled = torch.cat([pooled, _mean_pooler(hidden_states[layer])], dim=1)\n",
        "    return pooled\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEtwVyCmnnZi",
        "colab_type": "text"
      },
      "source": [
        "# **HUSE model**\n",
        "\n",
        "### HUSE_config:  \n",
        "A configuration class to hold configuration attributes for the HUSE model.  \n",
        "By default:\n",
        "- Some parameters are left to None, and are populated later.  \n",
        "- Some parameters have values from the HUSE paper.  \n",
        "- Some parameters have a dummy value.\n",
        "\n",
        "# <br>\n",
        "\n",
        "### ImageTower:\n",
        "The ImageTower module is modelled exactly as described in the HUSE paper.\n",
        "\n",
        "# <br>\n",
        "\n",
        "### TextTower:\n",
        "The TextTower module is modelled exactly as described in the HUSE paper.\n",
        "\n",
        "# <br>\n",
        "\n",
        "### HUSE_model:\n",
        "Uses instances of TextTower, ImageTower & a SharedLayer to completely model the HUSE model as described in the HUSE paper.  \n",
        "This model holds all the learnable parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erNgBxMnmlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HUSE_config():\n",
        "    def __init__(self):\n",
        "        self.tfidf_dim = None\n",
        "        self.num_classes = None \n",
        "        self.image_embed_dim = None\n",
        "        self.bert_hidden_dim = None\n",
        "        self.num_bert_layers = 4 # as in HUSE paper\n",
        "        self.image_tower_hidden_dim = 512 # as in HUSE paper\n",
        "        self.text_tower_hidden_dim = 512 # as in HUSE paper        \n",
        "\n",
        "        # parameters for Losses\n",
        "        # ---------- --- ------\n",
        "        self.alpha = 0.33 # weight for classificaion loss\n",
        "        self.beta = 0.33 # weight for semantic similarity loss\n",
        "        self.gamma = 0.33 # loss for cross modal loss\n",
        "        self.margin = 0.8 # relaxation parameter for semantic similarity loss\n",
        "\n",
        "    def __repr__(self):\n",
        "        string = []\n",
        "        for attr in dir(self):\n",
        "            if attr[0] != '_':\n",
        "                string.append(attr + ' : ' + str(eval('self.'+attr)))\n",
        "        return '\\n'.join(string)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "class ImageTower(nn.Module):\n",
        "    def __init__(self, config : HUSE_config):        \n",
        "        super(ImageTower, self).__init__()\n",
        "        self.drop_prob = 0.15\n",
        "        self.input_size = config.image_embed_dim\n",
        "        self.hidden_size  = config.image_tower_hidden_dim        \n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc3 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc4 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc5 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = F.dropout( F.relu( self.fc1( X ) ), p = self.drop_prob, training = self.training)\n",
        "        out = F.dropout( F.relu( self.fc2(out) ), p = self.drop_prob, training = self.training)\n",
        "        out = F.dropout( F.relu( self.fc3(out) ), p = self.drop_prob, training = self.training)\n",
        "        out = F.dropout( F.relu( self.fc4(out) ), p = self.drop_prob, training = self.training)\n",
        "        out = F.relu( self.fc5(out) )\n",
        "        out = F.normalize(out, p=2, dim=1) # L2 - Normalize\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TextTower(nn.Module):\n",
        "    def __init__(self, config : HUSE_config):        \n",
        "        super(TextTower, self).__init__()        \n",
        "        self.drop_prob = 0.15\n",
        "        self.input_size = config.tfidf_dim + config.bert_hidden_dim*config.num_bert_layers\n",
        "        self.hidden_size  = config.text_tower_hidden_dim        \n",
        "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, X):        \n",
        "        out = F.relu(self.fc1(X))\n",
        "        out = F.dropout(out, p = self.drop_prob, training = self.training)\n",
        "        out = F.relu( self.fc2(out) )\n",
        "        out = F.normalize(out, p=2, dim=1) # L2 - Normalize\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class HUSE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(HUSE, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        self.ImageTower = ImageTower(config)\n",
        "        self.TextTower = TextTower(config)\n",
        "        self.shared_fc_layer = nn.Linear(\n",
        "                in_features = config.image_tower_hidden_dim + config.text_tower_hidden_dim,\n",
        "                out_features = config.num_classes)\n",
        "        \n",
        "    def forward(self, image_embedding, text_embedding):        \n",
        "        out1 = self.ImageTower(image_embedding)\n",
        "        out2 = self.TextTower(text_embedding)\n",
        "        out = self.shared_fc_layer(torch.cat([out1, out2], dim=1))\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "def get_params_to_learn(model):\n",
        "    params_to_learn = []\n",
        "    for param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            params_to_learn.append(param)\n",
        "    return params_to_learn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9ACKhkbpwiC",
        "colab_type": "text"
      },
      "source": [
        "# **Losses**\n",
        "\n",
        "The HUSE paper describes 3 loss functions that are to be used so that the Embedding space has the required properties.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxEAw58-t3_I",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "###1. **Classification Loss:**  \n",
        "This is the softmax cross entropy loss. Implementing this in PyTorch is a 1-liner.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMFSulbEtzAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(ClassificationLoss, self).__init__()\n",
        "        \n",
        "    def forward(self, UE, labels):\n",
        "        '''\n",
        "        UE : embeddings created from model\n",
        "        target: index corresponding to class names\n",
        "        '''\n",
        "        loss = F.cross_entropy(UE, labels)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjLWh5d5tmw8",
        "colab_type": "text"
      },
      "source": [
        "###2. **Cross Modal Loss:**  \n",
        "This loss tries to minimize the universal embedding gap when an embedding for the same sample is created using either it's text representation, or it's image representation.  \n",
        "For CrossModalLoss, we get the universal embeddings from the HUSE_model using only either image or text at once.  \n",
        "- For obtaining universal embedding for text, we usa a Zero matrix { of appropriate shape } for representing the image embedding.\n",
        "- For obtaining universal embedding for image, we usa a Zero matrix { of appropriate shape } for representing the text embedding.  \n",
        "\n",
        "To get the loss, we take the cosine distance between them. Since these embeddings are coming from the same sample ( and only different modal ), minimizing the distance between them bring the embeddings from the 2 modals closer in universal embedding space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am7CacCStw1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossModalLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(CrossModalLoss, self).__init__()\n",
        "        \n",
        "    def forward(self, image_UE, text_UE):\n",
        "        '''\n",
        "        image_UE: universal embeddings created using only images\n",
        "        text_UE : universal embeddings created using only text\n",
        "        '''\n",
        "        loss = F.cosine_similarity(image_UE, text_UE).mean()\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQzDtrBt72c",
        "colab_type": "text"
      },
      "source": [
        "###3. **Semantic Similarity Loss:**  \n",
        "For demonstrating how I implemented this loss, here is a simple example:  \n",
        "\n",
        "        batch_size = 2\n",
        "        \n",
        "        We have 2 universal embedding and 2 labels in this batch.\n",
        "        UE = [UE_1 UE_2] # universal embeddings for batch samples\n",
        "        L =  [C_1 C_2]   # index corresponding to the class names of samples\n",
        "        \n",
        "        To calculate pairwise_embedding_distance, we create pairs of \n",
        "        samples in batch and calc their cosine distances.\n",
        "        UE_m = UE.repeat_interleave(2, -1)  # [ UE_1  UE_1  UE_2  UE_2 ]\n",
        "        UE_n = UE.repeat(2,1)               # [ UE_1  UE_2  UE_1  UE_2 ]\n",
        "        pairwise_embedding_distance = distance( UE_m, UE_n )\n",
        "        \n",
        "        Similarly pairwise label index are used to index into the Semantic Graph A\n",
        "        L_m = labels.repeat_interleave(2, -1)   # [ C_1  C_1  C_2  C_2 ]\n",
        "        L_n = labels.repeat(2,1)                # [ C_1  C_2  C_1  C_2 ]\n",
        "        \n",
        "        pairwise_class_distance = A[[L_m, L_n]]\n",
        "        '''\n",
        "\n",
        "- Once we get ```pairwise_embedding_distance``` & ```pairwise_class_distance``` (from the semantic graph), we calculate sigma using the equation:  \n",
        "![alt text](https://raw.githubusercontent.com/arpytanshu/HUSE-PyTorch/master/resources/SSLoss-sigma.png)\n",
        "\n",
        "- For getting the final loss, we again use the equation as describer in the HUSE paper.  \n",
        " loss = Σ ( σ * ( d (U_m,U_n) - Aij )² ) / N²\n",
        "![alt text](https://raw.githubusercontent.com/arpytanshu/HUSE-PyTorch/master/resources/SSLoss-equation.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgOe0hF8pzIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SemanticSimilarityLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self, margin, A, device):\n",
        "        '''\n",
        "        A : Semantic Graph created from the class name embeddings\n",
        "        margin : relaxation margin        \n",
        "        '''\n",
        "        super(SemanticSimilarityLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.A = A.to(device)\n",
        "        \n",
        "    def forward(self, UE, labels):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        UE : universal embeddings for samples in batch.\n",
        "        A  : tensor of size size [ num_classes x num_classes ]\n",
        "            Semantic Graph of distance between embedded class names\n",
        "        labels : index of sample's class name in Semantic Graph\n",
        "                 must have values between 0 & num_classes\n",
        "        '''\n",
        "        N = UE.shape[0] # batch size\n",
        "        \n",
        "        UE_m = UE.repeat_interleave(N, -2)\n",
        "        UE_n = UE.repeat(N,1)\n",
        "        pairwise_embedding_distance = F.cosine_similarity(UE_m, UE_n) # d (U_m , U_n )\n",
        "        \n",
        "        L_m = labels.repeat_interleave(N)\n",
        "        L_n = labels.repeat(N)\n",
        "        pairwise_class_distance = self.A[[L_m, L_n]]  # Aij\n",
        "\n",
        "        # print(pairwise_embedding_distance.device)\n",
        "        # print(pairwise_class_distance.device)\n",
        "        # print(self.margin.device)\n",
        "        # print(self.A.device)\n",
        "\n",
        "        sigma = self._calc_sigma(pairwise_class_distance, pairwise_embedding_distance)\n",
        "        # loss = Σ ( σ * (d(U_m,U_n) - Aij)² ) / N²\n",
        "        loss = (sigma * (pairwise_embedding_distance - pairwise_class_distance).pow(2)).mean() \n",
        "        return loss\n",
        "\n",
        "        \n",
        "    def _calc_sigma(self, pairwise_class_distance, pairwise_embedding_distance):\n",
        "        # calculate sigma as described in eq(10) in the HUSE paper.\n",
        "        sigma = ( pairwise_class_distance < self.margin ) & ( pairwise_embedding_distance < self.margin )\n",
        "        return sigma.to(torch.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PrCji8hqDkk",
        "colab_type": "text"
      },
      "source": [
        "### We wrap the 3 Loss modules into another module that also weights the 3 Losses using parameters from the HUSE_config:\n",
        "- HUSE_config.alpha\n",
        "- HUSE_config.beta\n",
        "- HUSE_config.gamma  \n",
        "![alt text](https://raw.githubusercontent.com/arpytanshu/HUSE-PyTorch/master/resources/combined_loss.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vt_oxyRqEIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingSpaceLoss(nn.Module):\n",
        "    def __init__(self, A, device, config : HUSE_config):\n",
        "        '''\n",
        "        config : A HUSE_config object containing model configuration parameters\n",
        "        Parameters\n",
        "        ----------\n",
        "        A     : Semantic Graph of distance between embedded class names\n",
        "        alpha : weight to control influence of Classification Loss\n",
        "        beta  : weight to control influence of Semantic Similarity Loss\n",
        "        gamma : weight to control influence of Cross Modal Loss\n",
        "        margin : relaxation margin used in Semantic Similarity Loss\n",
        "        '''\n",
        "        super(EmbeddingSpaceLoss, self).__init__()\n",
        "        self.config = config\n",
        "        self.Loss1 = ClassificationLoss()\n",
        "        self.Loss2 = SemanticSimilarityLoss(self.config.margin, A, device)\n",
        "        self.Loss3 = CrossModalLoss()\n",
        "\n",
        "    def forward(self, UE, UE_image, UE_text, labels):\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        UE : UE : universal embeddings for samples in batch.\n",
        "        UE_image : universal embeddings created using only image\n",
        "        UE_text : universal embeddings created using only text\n",
        "        labels : index corresponding to class names\n",
        "        '''\n",
        "        loss1 = self.config.alpha * self.Loss1(UE, labels)\n",
        "        loss2 = self.config.beta  * self.Loss2(UE, labels)\n",
        "        loss3 = self.config.gamma * self.Loss3(UE_image, UE_text)\n",
        "        loss = loss1 + loss2 + loss3\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMKG_9Jmw8Fn",
        "colab_type": "text"
      },
      "source": [
        "# **Train Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5h_kX8Aw_HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get dataframes, dataset and dataloader\n",
        "# --- ----------- ------- --- ----------\n",
        "tr_df = pd.read_csv(DATAFRAME_PATH)\n",
        "if DEV_MODE:\n",
        "    tr_df = tr_df.sample(NUM_DEV_SAMPLES)\n",
        "tr_df.reset_index(drop=True, inplace=True)\n",
        "tr_dataset = NAPDataset(tr_df.copy(),\n",
        "                          image_path = IMAGE_PATH,\n",
        "                          train_mode = True,\n",
        "                          bert_model_name = BERT_MODEL_NAME,\n",
        "                          transforms = image_transforms['train'],\n",
        "                          tokenizer = None) # also provide tokenizer if this is a test dataset\n",
        "tr_dataloader = DataLoader(tr_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "print('*** Dataloader object created. ***')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create Semantic Graph\n",
        "# ------ -------- -----\n",
        "print('*** Semantic Graph ...', end=' ')\n",
        "A = SemanticGraph(tr_dataset.classes_map)\n",
        "print('created. ***')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create image and text feature extraction models\n",
        "# ------ ----- --- ---- ------- ---------- ------\n",
        "Image_Embedding_Model, image_emb_dim = ImageEmbeddingModel('resnet18')\n",
        "Bert_Embedding_Model, bert_emb_dim = BertEmbeddingModel(BERT_MODEL_NAME)\n",
        "print('*** Image_Embedding_Model created. ***')\n",
        "print('*** Bert_Embedding_Model created. ***')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CONFIGURE dimensions in HUSE_config object\n",
        "# --------- ---------- -- ----------- ------\n",
        "huse_config = HUSE_config()\n",
        "huse_config.image_embed_dim = image_emb_dim\n",
        "huse_config.bert_hidden_dim = bert_emb_dim\n",
        "huse_config.tfidf_dim       = tr_dataset.tokenizer.num_words\n",
        "huse_config.num_classes     = tr_dataset.num_classes\n",
        "print('*** huse_config object created. ***')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create HUSE model, Loss, Optimizer & set PyTorch device\n",
        "# ------ ---- ------ ---- - -------- - --- ------- ------\n",
        "HUSE_model = HUSE(huse_config)\n",
        "EmbeddingLoss = EmbeddingSpaceLoss(A, device, huse_config)\n",
        "optimizer = RMSprop(HUSE_model.parameters(), lr=OPTIM_LEARNING_RATE, momentum=OPTIM_MOMENTUM)\n",
        "print('*** HUSE_model model created. ***')\n",
        "print('*** EmbeddingSpaceLoss object created. ***')\n",
        "print('*** optimizer created. ***')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "print('Device Type: ', device.type)\n",
        "if device.type == 'cuda':\n",
        "    HUSE_model.cuda()\n",
        "    Bert_Embedding_Model.cuda()\n",
        "    Image_Embedding_Model.cuda()\n",
        "    print('*** Moved models to cuda device. ***')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knl2aFSkxxHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(HUSE_model, dataloader, criterion, optimizer, num_epochs = 10):    \n",
        "    since = time.time()\n",
        "    train_loss_hist = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-'*10)\n",
        "        running_loss = 0.0\n",
        "        for batch_ix, batch in enumerate(dataloader):\n",
        "            batch_time = time.time()\n",
        "            # set device for training data\n",
        "            image = batch[0].to(device)\n",
        "            bert_input_ids = batch[1].to(device)\n",
        "            tfidf_vector = batch[2].to(device) \n",
        "            label = batch[3].to(torch.int64).to(device)\n",
        "            # get image & text embeddings \n",
        "            with torch.no_grad():\n",
        "                image_embedding = Image_Embedding_Model(image)\n",
        "                bert_embedding = bert_embeddings_pooler(Bert_Embedding_Model(bert_input_ids), huse_config.num_bert_layers)\n",
        "                text_embedding = torch.cat([bert_embedding, tfidf_vector], dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            HUSE_model.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "                universal_embedding = HUSE_model(image_embedding, text_embedding)\n",
        "                UnivEmb_only_image = HUSE_model(image_embedding, torch.zeros(text_embedding.shape, device=device))\n",
        "                UnivEmb_only_text  = HUSE_model(torch.zeros(image_embedding.shape, device=device), text_embedding)\n",
        "                \n",
        "                loss = criterion(universal_embedding, UnivEmb_only_image, UnivEmb_only_text, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * image.size(0)\n",
        "            batch_time_elapsed = time.time() - batch_time\n",
        "            if((batch_ix+1)%10 == 0):\n",
        "                print('epoch: {} batch: {}/{} batch_loss: {} time_in_batch: {}'.format(epoch+1, batch_ix+1, len(tr_dataloader), loss.item(), batch_time_elapsed))\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print('Loss: {}'.format(epoch_loss))\n",
        "        train_loss_hist.append(epoch_loss)\n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    return HUSE_model, train_loss_hist\n",
        "\n",
        "\n",
        "\n",
        "HUSE_model, hist = train_model(HUSE_model, tr_dataloader, EmbeddingLoss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjwaGoKzSbPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}